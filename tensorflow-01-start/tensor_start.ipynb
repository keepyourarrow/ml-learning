{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6332a2b7-06f9-4d0e-8c6c-39ba90546fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c624cbff-7809-4f01-92f1-95dbc87c4213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test)= keras.datasets.mnist.load_data()\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767f236e-b38b-4d26-9f0e-b538b080d005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bea9022970>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "plt.matshow(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f18d74-446c-4199-a705-91098b34acc9",
   "metadata": {},
   "source": [
    "## Since accuracy was below 50, we may want to reshape our numbers to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c48e380-2656-422a-93bb-5440810d66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a8800-edc8-4787-b5d4-4dff060687d5",
   "metadata": {},
   "source": [
    "## We need to reshape our 2d array into 1 dimensional array(from 2 columns to 1)\n",
    "1. From 28x28 to 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dac2465c-59d9-42b0-b6d3-fb70450fb4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), 28*28)\n",
    "X_test_flattened = X_test.reshape(len(X_test), 28*28)\n",
    "\n",
    "X_train_flattened.shape                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09a53693-7984-49f6-9795-aecc7c001305",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
       "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
       "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
       "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
       "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
       "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
       "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
       "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
       "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
       "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
       "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
       "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
       "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22fd4b-31da-4c8f-8ca5-a4be5bb77301",
   "metadata": {},
   "source": [
    "## Let's create a simple neurol network for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c10acce-f601-41ba-aa1a-886a46206e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 962us/step - loss: 0.4720 - accuracy: 0.8746\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 934us/step - loss: 0.3042 - accuracy: 0.9147\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 911us/step - loss: 0.2834 - accuracy: 0.9210\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 911us/step - loss: 0.2737 - accuracy: 0.9238\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 922us/step - loss: 0.2664 - accuracy: 0.9260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bea5c86fa0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train_flattened,y_train,epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a75dc-6fdc-42e3-b27f-0f8f54aa9d59",
   "metadata": {},
   "source": [
    "## Evaluate the data to get\n",
    "1. Loss\n",
    "1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4bad243-a1b6-4c6a-a43b-0d80c15bef3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 760us/step - loss: 0.2669 - accuracy: 0.9262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2668556571006775, 0.9261999726295471]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_flattened, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6d6b98b-f740-4e63-b37e-eb805cbaaccf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.4587095e-02, 4.1716936e-07, 5.2373320e-02, 9.4337773e-01,\n",
       "        1.6084909e-03, 9.1861129e-02, 1.2662430e-06, 9.9973452e-01,\n",
       "        7.2426498e-02, 5.8539915e-01],\n",
       "       [3.6182559e-01, 5.5094659e-03, 9.9939072e-01, 3.1038287e-01,\n",
       "        4.6887710e-10, 8.6757720e-01, 8.2940876e-01, 8.8264303e-13,\n",
       "        1.0048461e-01, 1.3352073e-09],\n",
       "       [4.1323900e-04, 9.9413681e-01, 6.4307708e-01, 2.9427880e-01,\n",
       "        2.4581432e-02, 1.0984129e-01, 1.3745385e-01, 1.1792919e-01,\n",
       "        3.5404068e-01, 5.1223069e-02],\n",
       "       [9.9962735e-01, 3.1553189e-08, 1.0125694e-01, 7.5000525e-03,\n",
       "        4.9159116e-05, 1.5668872e-01, 1.0130012e-01, 1.6006142e-02,\n",
       "        1.9903839e-02, 1.0893345e-02],\n",
       "       [6.3365966e-02, 3.9891289e-05, 1.3832313e-01, 4.2584538e-03,\n",
       "        9.8963368e-01, 1.7050415e-02, 1.4387789e-01, 2.3103201e-01,\n",
       "        3.0111122e-01, 6.3999504e-01]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = model.predict(X_test_flattened)\n",
    "y_predicted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bced57f-6ce5-48fe-9d6b-008c153cc8f2",
   "metadata": {},
   "source": [
    "## Convert float numbers to whole numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ddd716-3e7c-41bf-be62-05a39566ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [np.argmax(i) for i in y_predicted] \n",
    "predicted_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741612a9-07d6-4de6-8f0c-ca5f51a1a203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.math.confusion_matrix(labels=y_test,predictions=predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c632a205-421f-4ccb-8161-22cbc2d4616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "print(ConfusionMatrixDisplay.from_predictions(y_test,predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc094858-2216-4c97-862f-43d14770646e",
   "metadata": {},
   "source": [
    "## Let's add a hidden layer to up the performance\n",
    "1. The more hidden layers you have the longer it'd take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d3289-2012-464b-83f3-31118ac41825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, input_shape=(784,), activation='relu'),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', ## another optimized SGD\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "H = model.fit(X_train_flattened,y_train,epochs=38,batch_size=128, validation_data=(X_test_flattened, y_test),) # adjust batch size, epochs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b75e7cf-504b-488f-a7fe-ce6ac32e696a",
   "metadata": {},
   "source": [
    "## Another way of doing it \n",
    "# model = keras.Sequential([\n",
    "    # keras.layers.Dense(500, input_shape=(784,), activation='relu'),\n",
    "    # keras.layers.Dense(250, activation='relu'), # no need to specify shape here now, it'd do it on its own\n",
    "   # keras.layers.Dense(10, activation='sigmoid')\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad97d8-7609-40e5-877d-01535950542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_flattened, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e20210c-acad-4772-97d5-58c6ae64249a",
   "metadata": {},
   "source": [
    "## Yay, it improved by 5 percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1700d0-4597-4023-860e-f2f11ef2df61",
   "metadata": {},
   "source": [
    "## Let's graph the history that we get from our model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea99d75-3550-484a-8d7b-fc66ac99199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 38), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 38), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 38), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 38), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3544967-bf47-44d8-8e41-5799ec667f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test_flattened,batch_size=128) # batch size isn't a required parameter, but it might make things faster (processing wise)\n",
    "predicted_labels = [np.argmax(i) for i in y_predicted] \n",
    "\n",
    "\n",
    "print(ConfusionMatrixDisplay.from_predictions(y_test,predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596f6bd-6f01-40ac-98fd-69b5976cfc61",
   "metadata": {},
   "source": [
    "## How to optimize \n",
    "1. Adjust batch sizes,epochs, add amount of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77570c0e-4142-4bb4-b3c1-589709a83f2c",
   "metadata": {},
   "source": [
    "## If you don't want to flatten the array, Keras comes with a hidden layer for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f52f902a-fd97-48a4-a20c-69f4b361d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2737 - accuracy: 0.9229\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1205 - accuracy: 0.9648\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0837 - accuracy: 0.9750\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0649 - accuracy: 0.9801\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0521 - accuracy: 0.9839\n",
      "Wall time: 11.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bea6eff6a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)), # flattens it here (supply an image shape)\n",
    "    keras.layers.Dense(100, activation='relu'), # no need to specify shape here now, it'd do it on its own\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# not flattened now\n",
    "model.fit(X_train,y_train,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea862545-ec7a-4745-a044-06ae15247545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 871us/step - loss: 0.0851 - accuracy: 0.9735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08510109782218933, 0.9735000133514404]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test) # not flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f1acf-13de-46e7-a498-0676003ecbb4",
   "metadata": {},
   "source": [
    "## Same result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aadf69-e397-4e91-9521-63cee44e4987",
   "metadata": {},
   "source": [
    "## Always shoot for the high accuracy. \n",
    "1. 92% might have been great 20 years ago, but with such powerful tools like CNN we can easily get 98,99%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32ca09-ef89-4625-b776-789f6ce6605f",
   "metadata": {},
   "source": [
    "## Activate a certain CPU/GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8b297-3dfd-45e0-b885-113eabdde494",
   "metadata": {},
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc4c9baf-e48a-4217-8c6d-2dd834ec1648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2764 - accuracy: 0.9218\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 980us/step - loss: 0.1273 - accuracy: 0.9625\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 952us/step - loss: 0.0896 - accuracy: 0.9736\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0698 - accuracy: 0.9785\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 954us/step - loss: 0.0537 - accuracy: 0.98400s - loss: 0.0539 - accuracy: 0.\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device(\"CPU:0\"): # this Line\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28,28)), # flattens it here (supply an image shape)\n",
    "        keras.layers.Dense(100, activation='relu'), # no need to specify shape here now, it'd do it on its own\n",
    "        keras.layers.Dense(10, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # not flattened now\n",
    "    model.fit(X_train,y_train,epochs=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf0165-e299-4389-8856-cc9faf31e8af",
   "metadata": {},
   "source": [
    "## How to use table in md\n",
    "| Age | Weight |\n",
    "|:----|:-----|\n",
    "| 5 | 30 |\n",
    "| 6 | 45 |\n",
    "| 7 | 60 |\n",
    "| 8 | 75 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180cc18-d04c-4a81-be01-037c13bfa8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
